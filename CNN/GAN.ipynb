{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对抗生成网络----GAN\n",
    "- 1. GANs实际是有两个网络, 一个生成网络 $G$ ，一个辨别网络 $D$, 两者相互竞争。生成网络伪造数据传入辨别网络。辨别网络同时也接收真实数据，并判断所输入数据是真实的还是伪造的。生成网络持续训练为了躲过辨别网络的鉴别，它试图输出看上去像真实数据的数据。而辨别网络持续训练来分辨谁真谁假！ 最终平衡的结果是：生成网络伪造的数据让辨别网络无法区分。\n",
    "![GAN diagram](images/gan_diagram.png)\n",
    " 2. 上图是GANs一般的网络结构，使用 MNIST图片作为真实数据，潜在样本是一个随机向量，生成网络据此开始伪造图片。随着生成网络学习深入，它会知道如何从随机向量映射到可辨识的图片，从而愚弄辨别网络。\n",
    " 3. 辨别网络最终输出是一个sigmoid激活值,0代表伪造图片，1代表真实图片。 如果我们只关心生成新的图片，我们可以在训练完成后丢掉辨识网络即可。\n",
    " - 网络结构\n",
    " ![GAN diagram](images/gan_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import os\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"x_train data shape: \", x_train.shape)\n",
    "print(\"y_train data shape: \", y_trian.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 转换数据格式\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "x_train = tf.expand_dims(x_train, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 构建生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator():\n",
    "    input_x = tf.keras.Input((100,))\n",
    "    fc = tf.keras.layers.Dense(784, activation=None)(input_x)\n",
    "    fc = tf.keras.layers.LeakyRelu()(fc)\n",
    "    output = tf.keras.activations.tanh()(fc)\n",
    "    model = tf.keras.Model(inputs=input_x, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 构建判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.Dense(128, activation=None))\n",
    "    model.add(tf.keras.layers.LeakyRelu())\n",
    "    model.add(tf.keras.layers.Dense(1, activation=None))\n",
    "    # model.add(tf.keras.activations.sigmoid())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_losses(real_output, fake_output):\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    d_real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    d_fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    d_loss = d_real_loss + d_fake_loss\n",
    "    return d_loss\n",
    "\n",
    "def gene_losses(fake_output):\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    g_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "generator_optimizer = tf.keras.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = model_generator()\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = model_discriminator()\n",
    "result = discriminator(generated_image)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./models/\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoints = tf.train.Checkpoint(discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator_optimizer=generator_optimizer,\n",
    "                                 discriminator=discriminator,\n",
    "                                 generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, batch_size, noise_dim):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    with tf.GradientTape() as dis_tape, tf.GradientTape() as gen_tape:\n",
    "        gene_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(gene_images, training=True)\n",
    "        ## loss\n",
    "        d_loss = dis_losses(real_output, fake_output)\n",
    "        g_loss = gene_losses(fake_output)\n",
    "    grad_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_discriminator = dis_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grad_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_discriminator, discriminator.trainbale_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, epoches, batch_size, noise_dim):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(256).batch(batch_size)\n",
    "    for epoch in epoches:\n",
    "        for batch in train_dataset:\n",
    "            train_step(batch, batch_size, noise_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
