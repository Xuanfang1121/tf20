{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TextCNN\n",
    "- ## TextCNN模型介绍\n",
    "   卷积神经网络CNN在计算机视觉方向应用很广，解决了CV方向很多的问题。在2014年Yoon Kim针对CNN的输入层做了一些变形，提出了文本分类模型textCNN，与传统图像的CNN网络相比, textCNN 在网络结构上没有任何变化(甚至更加简单了), 卷积层数减少，模型更简单。\n",
    "   - TextCNN模型demo\n",
    " <img src=\"./images/textcnn3.png\" style=\"height: 60%;width: 60%; position: relative;right:10%\">\n",
    "   - TextCNN模型说明 <br />\n",
    "       - 这里的卷积神经网络CNN是1D的<br />\n",
    "       - 卷积核的宽度与词的维度一致，卷积核的移动方向是上下移动<br />\n",
    "       - 卷积后得到的卷积层的大小为 n - h + 1<br />\n",
    "           其中： n是词的个数，h是卷积核的大小\n",
    "\n",
    "   - TextCNN优点\n",
    "       - 参数少，运行速度快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMDB数据集介绍\n",
    "IMDB数据集有50000条数据集，训练集有25000条，测试集有25000条。数据label分为正负类，每类各占50%。这里采用keras的数据集中的imdb，每条数据已经做好了编码。label为0表示负类，label为1表示正类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 搭建TextCNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载imdb数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self, embedding_size, max_feature, max_length, drop_rate, num_classes):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_feature = max_feature\n",
    "        self.max_length = max_length\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_classes = num_classes\n",
    "    def build_model(self):\n",
    "        input_x = tf.keras.Input((self.max_length,))\n",
    "        embedding = tf.keras.layers.Embedding(input_dim=self.max_feature, out_dim=self.embedding_size, \n",
    "                                            input_length=self.max_length)(input_x)\n",
    "        convs = []\n",
    "        kernel_list = [3, 4, 5]\n",
    "        for kernel_size in kernel_list:\n",
    "            conv = tf.keras.layers.Conv1D(filters=64, kernel_size=kernel_size, strides=(1, 1),\n",
    "                                          padding=\"valid\", activate=\"relu\")(embedding)\n",
    "            pool = tf.keras.layers.GlobalMaxPool1D()(conv)\n",
    "            convs.append(pool)\n",
    "        con = tf.keras.layers.Concatenate()(convs)\n",
    "        fc = tf.keras.layers.Dense(64, activation=\"relu\")(con)\n",
    "        fc = tf.keras.layers.Dropout(self.drop_rate)(fc)\n",
    "        output = tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")(fc)\n",
    "        model = tf.keras.Model(inputs=input_x, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 9s 1us/step\n",
      "train data shape:  (25000,)\n",
      "train label shape:  (25000,)\n",
      "test data shape:  (25000,)\n",
      "test labbel shape:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=3000)\n",
    "print(\"train data shape: \", x_train.shape)\n",
    "print(\"train label shape: \", y_train.shape)\n",
    "print(\"test data shape: \", x_test.shape)\n",
    "print(\"test labbel shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96356 679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq_length = 300\n",
    "sen_length = [len(x_train[i]) for i in range(x_train.shape[0])]\n",
    "sen_length = np.array(sen_length)\n",
    "lengths = int(np.mean(sen_length) + 2.5 * np.std(sen_length))\n",
    "precent = np.sum(sen_length < lengths) / len(sen_length)\n",
    "print(precent, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (25000, 679)\n",
      "test data shape:  (25000, 679)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=lengths)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=lengths)\n",
    "print(\"train data shape: \", x_train.shape)\n",
    "print(\"test data shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label:  (25000, 2)\n",
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.one_hot(y_train, 2)\n",
    "y_test = tf.one_hot(y_test, 2)\n",
    "print(\"train label: \", y_train.shape)\n",
    "print(y_train[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 搭建textcnn模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_size = 300\n",
    "max_feature = 3000\n",
    "drop_rate = 0.2\n",
    "num_class = 2\n",
    "epoches = 20\n",
    "batch_size = 128\n",
    "\n",
    "if not os.path.exists(\"./models/textcnn/\"):\n",
    "    os.mkdir(\"./models/textcnn/\")\n",
    "if not os.path.exists(\"./logs_textcnn/\"):\n",
    "    os.mkdir(\"./logs_textcnn/\")\n",
    "log_dir = \"./logs_textcnn/textcnn_event-{}\".format(int(time.time()))\n",
    "model_path = os.path.join(\"./models/textcnn/textcnn.h5\")\n",
    "my_callbacks = [tf.keras.callbacks.ModelCheckpoint(model_path, verbose=1),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "                tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "model = TextCNN(embedding_size, max_feature, lengths, drop_rate, num_class).build_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epoches, \n",
    "                    callbacks=my_callbacks, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "fig1.savefig('./image/loss_textcnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], \"r\", linewidth=3.0)\n",
    "plt.plot(history.history[\"val_accuracy\"], \"b\", linewidth=3.0)\n",
    "plt.legend([\"train acc\", \"val_acc\"], fontsize=18)\n",
    "plt.xlabel(\"epoch\", fontsize=16)\n",
    "plt.ylabel(\"accuracy\", fontsize=16)\n",
    "plt.title(\"accuracy curve\")\n",
    "plt.savefig(\"./image/textcnn_accuracy.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu20",
   "language": "python",
   "name": "tensorflowgpu20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
