{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager Execution是动态图，是一种命令式交互编程环境。\n",
    "- 无需构建图，操作会返回具体的值，与tensorflow1.x的相比，不需要构建图在运行计算图，即sess.run()\n",
    "- 自然控制流程 - 使用 Python 控制流程而不是图控制流程，简化了动态模型的规范\n",
    "- 使用 Python 数据结构，与numpy兼容\n",
    "- 执行效率高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow的默认运行方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eager Execution运算\n",
    "- tf.Tensor.numpy方法将对象的值作为NumPy的ndarray类型返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3  4  6  8]\n",
      " [10 12  9 13]], shape=(2, 4), dtype=int32)\n",
      "x: [[ 3  4  6  8]\n",
      " [10 12  9 13]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 4, 6], [8, 10, 7, 11]])\n",
    "x = tf.add(x, 2)\n",
    "print(x)\n",
    "print(\"x:\", x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eager Execution 动态控制流\n",
    "- Eager Execution再执行模型时可以使用Python的所有功能，例如for, if等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_factor(num):\n",
    "    result = []\n",
    "    num = tf.convert_to_tensor(num)\n",
    "    for i in range(1, num.numpy()+1):\n",
    "        if int(num.numpy()) % i == 0:\n",
    "            result.append(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all factors: [1, 2, 7, 14]\n"
     ]
    }
   ],
   "source": [
    "result = find_factor(14)\n",
    "print(\"all factors:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eager Execution梯度计算\n",
    "在tensorflow1.x版本是静态图，每一个静态图包含前向图和反向图。反向图用于梯度计算，用于训练模型的过程。在tensorflow2.x版本中默认运行的模式式Eager，用tf.GradientTape()计算函数和变量的梯度，类似于一个连接器的作用。tf.GradientTape()是官方推荐的用法。\n",
    "- tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "   - persistent：默认False，表示调用gradient函数后释放，无法进行下一次的梯度计算，如果进行多次梯度计算，设persistent为True\n",
    "   - watch_accessed_variables：默认True，自动监测变量，对变量进行求导<br />\n",
    "   \n",
    "- gradient(target,\n",
    "    sources,\n",
    "    output_gradients=None,\n",
    "    unconnected_gradients=tf.UnconnectedGradients.NONE) \n",
    "    - target: 可微的列表，变量，嵌套的张量结构\n",
    "    - sources： 列表，变量，嵌套的张量结构，target在sources处的微分\n",
    "    - output_gradients： 输出梯度的列表，梯度的每一个元素，默认为None\n",
    "    - unconnected_gradients：如果sources与target没有链接(不可微),返回\"none\"或者\"zero\"，UnconnectedGradients会有详细的说明,并且默认为\"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子1：计算 y = 3x^2 在 x = 2的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算的梯度为 tf.Tensor(18.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = 3 * x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "print(\"计算的梯度为\", dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \\* watch的作用是把常量x加进来，tf.GradientTape()默认监控到由tf.Variable定义的可训练的变量。对常量求梯度也可以设置watch_accessed_variables=False<br />\n",
    " \\* 常量类型是float型，如果是int，会返回none<br />\n",
    " \\* 一般在网络中使用时，不需要显式调用watch函数，使用默认设置，GradientTape会监控可训练变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子2 计算y=3x^3在x=2的二阶梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.GradientTape()中的参数persistent为默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-74a501d0e4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdy_dx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdy2_dx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dy_dx: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy_dx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dy2_dx2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy2_dx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowgpu20\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \"\"\"\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[0;32m    966\u001b[0m                          \"non-persistent tapes.\")\n\u001b[0;32m    967\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = 3 * x * x * x\n",
    "    z = x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "    dy2_dx2 = g.gradient(z, x)\n",
    "print(\"dy_dx: \", dy_dx)\n",
    "print(\"dy2_dx2: \", dy2_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.GradientTape()中的参数persistent设为True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "dy_dx:  tf.Tensor(81.0, shape=(), dtype=float32)\n",
      "dy2_dx2:  tf.Tensor(54.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = 3 * x * x * x\n",
    "    dy_dx = g.gradient(y, x)\n",
    "    dy2_dx2 = g.gradient(dy_dx, x)\n",
    "print(\"dy_dx: \", dy_dx)\n",
    "print(\"dy2_dx2: \", dy2_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mathod2： 用两个tf.GradientTape()求梯度，采用默认的persistent=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dx:  tf.Tensor(81.0, shape=(), dtype=float32)\n",
      "d2y_dx2 tf.Tensor(54.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = 3 * x * x * x     \n",
    "    dy_dx = gg.gradient(y, x)     # Will compute to 81.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 54.0\n",
    "print(\"dy_dx: \", dy_dx)\n",
    "print(\"d2y_dx2\", d2y_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子3 优化----求解参数w，b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimLayers(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(tf.random.normal((6, 1)))\n",
    "        self.b = tf.Variable(tf.random.normal([10]))\n",
    "        super(SimLayers, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = tf.add(tf.matmul(inputs, self.w), self.b)\n",
    "        output = tf.keras.activations.sigmoid(output)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x shape:  (10, 6)\n",
      "step: 10 loss: 1.0718059539794922\n",
      "step: 20 loss: 1.0698118209838867\n",
      "step: 30 loss: 1.0679030418395996\n",
      "step: 40 loss: 1.0660481452941895\n",
      "step: 50 loss: 1.0642211437225342\n",
      "step: 60 loss: 1.0623986721038818\n",
      "step: 70 loss: 1.0605601072311401\n",
      "step: 80 loss: 1.05868661403656\n",
      "step: 90 loss: 1.0567595958709717\n",
      "step: 100 loss: 1.0547617673873901\n",
      "step: 110 loss: 1.0526756048202515\n",
      "step: 120 loss: 1.0504834651947021\n",
      "step: 130 loss: 1.0481678247451782\n",
      "step: 140 loss: 1.0457106828689575\n",
      "step: 150 loss: 1.043094277381897\n",
      "step: 160 loss: 1.0403008460998535\n",
      "step: 170 loss: 1.0373129844665527\n",
      "step: 180 loss: 1.0341147184371948\n",
      "step: 190 loss: 1.0306910276412964\n",
      "step: 200 loss: 1.0270297527313232\n",
      "step: 210 loss: 1.0231202840805054\n",
      "step: 220 loss: 1.018954873085022\n",
      "step: 230 loss: 1.0145275592803955\n",
      "step: 240 loss: 1.009832739830017\n",
      "step: 250 loss: 1.0048632621765137\n",
      "step: 260 loss: 0.9996078610420227\n",
      "step: 270 loss: 0.9940468072891235\n",
      "step: 280 loss: 0.9881490468978882\n",
      "step: 290 loss: 0.9818704724311829\n",
      "step: 300 loss: 0.9751551151275635\n",
      "step: 310 loss: 0.9679424166679382\n",
      "step: 320 loss: 0.9601860642433167\n",
      "step: 330 loss: 0.9518821239471436\n",
      "step: 340 loss: 0.9431057572364807\n",
      "step: 350 loss: 0.934038519859314\n",
      "step: 360 loss: 0.9249594807624817\n",
      "step: 370 loss: 0.9161930084228516\n",
      "step: 380 loss: 0.9080289602279663\n",
      "step: 390 loss: 0.900661826133728\n",
      "step: 400 loss: 0.8941734433174133\n",
      "step: 410 loss: 0.8885518908500671\n",
      "step: 420 loss: 0.8837272524833679\n",
      "step: 430 loss: 0.8796027302742004\n",
      "step: 440 loss: 0.8760764598846436\n",
      "step: 450 loss: 0.8730531930923462\n",
      "step: 460 loss: 0.8704495429992676\n",
      "step: 470 loss: 0.8681949377059937\n",
      "step: 480 loss: 0.8662310242652893\n",
      "step: 490 loss: 0.8645094037055969\n",
      "step: 500 loss: 0.8629915118217468\n",
      "optimizer weights: [[ 0.48703483]\n",
      " [-0.6009448 ]\n",
      " [-0.49283025]\n",
      " [-0.43430007]\n",
      " [ 2.1188369 ]\n",
      " [-0.31776077]]\n",
      "optimizer bias: [-0.3663807   1.2744135   0.76765925  0.09824821  0.5786315  -1.5743871\n",
      "  0.23810484 -0.06116868  0.01710841  1.2173069 ]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(10, 6))\n",
    "y = tf.random.normal([10])\n",
    "## 对x进行加噪声处理\n",
    "x = tf.multiply(x, 2.5) + tf.random.normal(shape=(10, 6))\n",
    "print(\"input x shape: \", x.shape)\n",
    "## 加载模型\n",
    "model = SimLayers()\n",
    "## 优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "## 训练过程\n",
    "for i in range(500):\n",
    "    ## 计算梯度\n",
    "    with tf.GradientTape() as tape:\n",
    "        ## 计算loss\n",
    "        loss = model(x) - y\n",
    "        losses = tf.reduce_mean(tf.square(loss))\n",
    "        grad = tape.gradient(losses, [model.w, model.b])\n",
    "    optimizer.apply_gradients(zip(grad, [model.w, model.b]))\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"step: {} loss: {}\". format(i+1, losses))\n",
    "        \n",
    "print(\"optimizer weights: {}\".format(model.w.numpy()))\n",
    "print(\"optimizer bias: {}\".format(model.b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子4 GPU和CPU性能比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 28x28x1\n",
    "def CNNModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, kernel_initializer='he_normal', strides=1, activation='relu', padding='same',\n",
    "                                         name=\"conv1\"))\n",
    "    model.add(tf.keras.layers.MaxPool2D((2, 2), strides=2, padding='valid', name=\"pool1\"))\n",
    "    model.add(tf.keras.layers.Conv2D(128, 3, kernel_initializer=\"he_normal\", strides=1, activation=\"relu\", padding=\"same\",\n",
    "                                    name=\"conv2\"))\n",
    "    model.add(tf.keras.layers.MaxPool2D((2, 2), strides=2, padding=\"valid\", name=\"pool2\"))\n",
    "    model.add(tf.keras.layers.Conv2D(128, 3, kernel_initializer=\"he_normal\", strides=1, activation=\"relu\", padding=\"same\",\n",
    "                                    name=\"conv3\"))\n",
    "    model.add(tf.keras.layers.MaxPool2D((2, 2), strides=2, padding=\"valid\", name=\"pool3\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax',kernel_initializer='he_normal'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train_gpu():\n",
    "    (train_data, train_label), (test_data, test_label) = tf.keras.datasets.mnist.load_data()\n",
    "    ## 转换数据格式\n",
    "    train_data = train_data.astype('float32')/255\n",
    "    test_data = test_data.astype(\"float32\")/255\n",
    "    ## 对label进行one-hot编码\n",
    "    train_label = tf.keras.utils.to_categorical(train_label, 10)\n",
    "    train_label = tf.convert_to_tensor(train_label)\n",
    "    test_label = tf.keras.utils.to_categorical(test_label, 10)\n",
    "    test_label= tf.convert_to_tensor(test_label)\n",
    "    ##增加维度\n",
    "    train_data = tf.expand_dims(train_data, -1)\n",
    "    test_data = tf.expand_dims(test_data, -1)\n",
    "    print(\"train data shape\", train_data.shape)\n",
    "    model = CNNModel()\n",
    "    my_callbacks = [tf.keras.callbacks.ModelCheckpoint('./logs/cnn_model.h5', verbose=1),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, mode='max')]\n",
    "    ## 训练模型\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        start_time = time.time()\n",
    "        model.fit(train_data, train_label, batch_size=64, verbose=1, epochs=3, callbacks=my_callbacks, \n",
    "                  validation_data=(test_data, test_label)) #\n",
    "        diff = time.time() - start_time\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cpu():\n",
    "    (train_data, train_label), (test_data, test_label) = tf.keras.datasets.mnist.load_data()\n",
    "    ## 转换数据格式\n",
    "    train_data = train_data.astype('float32')/255\n",
    "    test_data = test_data.astype(\"float32\")/255\n",
    "    ##增加维度\n",
    "    train_data = tf.expand_dims(train_data, -1)\n",
    "    test_data = tf.expand_dims(test_data, -1)\n",
    "    print(\"train data shape\", train_data.shape)\n",
    "    ## 对label进行one-hot编码\n",
    "    train_label = tf.one_hot(train_label, 10)\n",
    "    test_label = tf.one_hot(test_label, 10)\n",
    "    model = CNNModel()\n",
    "    my_callbacks = [tf.keras.callbacks.ModelCheckpoint('./logs/cnn_model.h5', verbose=1),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, mode='max')]\n",
    "    ## 训练模型\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        start_time = time.time()\n",
    "        model.fit(train_data, train_label, batch_size=64, verbose=1, epochs=3, callbacks=my_callbacks, \n",
    "                  validation_data=(test_data, test_label))\n",
    "        diff = time.time() - start_time\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (60000, 28, 28, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9612 ETA:  - ETA: 4s -\n",
      "Epoch 00001: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 10s 174us/sample - loss: 0.1260 - accuracy: 0.9612 - val_loss: 0.0432 - val_accuracy: 0.9858\n",
      "Epoch 2/3\n",
      "59584/60000 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9881\n",
      "Epoch 00002: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0382 - accuracy: 0.9881 - val_loss: 0.0301 - val_accuracy: 0.9893\n",
      "Epoch 3/3\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9912\n",
      "Epoch 00003: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0275 - accuracy: 0.9912 - val_loss: 0.0379 - val_accuracy: 0.9876\n",
      "train data shape (60000, 28, 28, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9616\n",
      "Epoch 00001: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.1268 - accuracy: 0.9616 - val_loss: 0.0403 - val_accuracy: 0.9862\n",
      "Epoch 2/3\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9875\n",
      "Epoch 00002: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.0307 - val_accuracy: 0.9902\n",
      "Epoch 3/3\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9907\n",
      "Epoch 00003: saving model to ./logs/cnn_model.h5\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.0278 - accuracy: 0.9907 - val_loss: 0.0264 - val_accuracy: 0.9913\n",
      "gpu training takes time:  27.836952447891235\n",
      "cpu training takes time:  184.38331365585327\n"
     ]
    }
   ],
   "source": [
    "gpu_time = train_gpu()\n",
    "cpu_time = train_cpu()\n",
    "print(\"gpu training takes time: \", gpu_time)\n",
    "print(\"cpu training takes time: \", cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
